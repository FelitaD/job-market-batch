{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Streamlined ingestion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/donor/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "from langdetect import detect\n",
    "\n",
    "from config.definitions import JOB_MARKET_DB_USER, JOB_MARKET_DB_PWD\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [],
   "source": [
    "db_string = f\"postgresql://{JOB_MARKET_DB_USER}:{JOB_MARKET_DB_PWD}@localhost:5432/job_market\"\n",
    "engine = create_engine(db_string)\n",
    "jobs = pd.read_sql(\"jobs\", engine)\n",
    "# de_jobs = pd.read_sql(\"SELECT * FROM jobs WHERE title LIKE '%data engineer%';\", engine)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    url  \\\n0     https://datai.jobs/job/databricks-manager-big-...   \n1     https://datai.jobs/job/argo-software-engineeri...   \n2     https://datai.jobs/job/lyft-data-engineer-kyiv...   \n3     https://datai.jobs/job/tier-senior-data-engine...   \n4     https://datai.jobs/job/arrival-text1-saint-pet...   \n...                                                 ...   \n1693  https://www.welcometothejungle.com/fr/companie...   \n1694  https://www.welcometothejungle.com/fr/companie...   \n1695  https://www.welcometothejungle.com/fr/companie...   \n1696  https://www.welcometothejungle.com/fr/companie...   \n1697  https://www.welcometothejungle.com/fr/companie...   \n\n                                                  title  \\\n0                          Manager Big Data Engineering   \n1     Software Engineering Manager, Reporting & Data...   \n2                                  Data Engineer – Kyiv   \n3                                  Senior Data Engineer   \n4                                  Senior Data Engineer   \n...                                                 ...   \n1693                           Lead Data Engineer (H/F)   \n1694                          Data Engineer Confirmé(e)   \n1695                                  Data Engineer H/F   \n1696               Data Engineer - Plateforme Big Data    \n1697        Data Engineer Junior H/F - Data & Analytics   \n\n                                             company  \\\n0                                         Databricks   \n1                                            Argo AI   \n2                                               Lyft   \n3                                      TIER Mobility   \n4                                            Arrival   \n...                                              ...   \n1693                                    Voyage Privé   \n1694                                       Linkvalue   \n1695                                          Inetum   \n1696  Assistance Publique - Hôpitaux de Paris - DSI    \n1697                             BearingPoint France   \n\n                                   location       type  \\\n0                           Munich, Germany  Full Time   \n1                           Munich, Germany  Full Time   \n2                             Kyiv, Ukraine  Full Time   \n3     Amsterdam, North Holland, Netherlands  Full Time   \n4          Saint Petersburg Yasnaya Polyana  Full Time   \n...                                     ...        ...   \n1693                        Aix-En-Provence        CDI   \n1694                                  Paris        CDI   \n1695                             Courbevoie        CDI   \n1696                                  Paris        CDI   \n1697                       Paris La Défense        CDI   \n\n                                               industry  \\\n0                                          Data Science   \n1                        Vehicles & Autonomous Mobility   \n2                        Vehicles & Autonomous Mobility   \n3                        Vehicles & Autonomous Mobility   \n4                        Vehicles & Autonomous Mobility   \n...                                                 ...   \n1693                          E-commerce, Loisirs, Luxe   \n1694                            IT / Digital, Logiciels   \n1695                                       IT / Digital   \n1696  Big Data, Intelligence artificielle / Machine ...   \n1697  Organisation / Management, Stratégie, Transfor...   \n\n                            remote created_at  \\\n0                          Inconnu 2021-12-27   \n1                          Inconnu 2021-12-27   \n2                          Inconnu 2021-12-27   \n3                          Inconnu 2021-12-27   \n4                          Inconnu 2021-12-27   \n...                            ...        ...   \n1693                       Inconnu 2022-01-25   \n1694                       Inconnu 2022-01-25   \n1695                       Inconnu 2022-01-25   \n1696  Télétravail partiel possible 2022-01-25   \n1697                       Inconnu 2022-01-25   \n\n                                                   text  \n0     \n            At Databricks we work on some of ...  \n1     \n            Company: Argo AI GmbH\nWho we are:...  \n2     \n            At Lyft, our mission is to improv...  \n3     \n            \nTHIS IS US\nTIER Mobility is Euro...  \n4     \n            At Arrival, our team is creating ...  \n...                                                 ...  \n1693  Aventure entrepreneuriale lancée en France en ...  \n1694  L’ambition de Romain et Thibault, les co-fonda...  \n1695  Inetum est une ESN agile, une société de servi...  \n1696  L’ Assistance Publique - Hôpitaux de Paris (AP...  \n1697  BearingPoint  est un cabinet de conseil en man...  \n\n[1698 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>title</th>\n      <th>company</th>\n      <th>location</th>\n      <th>type</th>\n      <th>industry</th>\n      <th>remote</th>\n      <th>created_at</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://datai.jobs/job/databricks-manager-big-...</td>\n      <td>Manager Big Data Engineering</td>\n      <td>Databricks</td>\n      <td>Munich, Germany</td>\n      <td>Full Time</td>\n      <td>Data Science</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>At Databricks we work on some of ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://datai.jobs/job/argo-software-engineeri...</td>\n      <td>Software Engineering Manager, Reporting &amp; Data...</td>\n      <td>Argo AI</td>\n      <td>Munich, Germany</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>Company: Argo AI GmbH\nWho we are:...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://datai.jobs/job/lyft-data-engineer-kyiv...</td>\n      <td>Data Engineer – Kyiv</td>\n      <td>Lyft</td>\n      <td>Kyiv, Ukraine</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>At Lyft, our mission is to improv...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://datai.jobs/job/tier-senior-data-engine...</td>\n      <td>Senior Data Engineer</td>\n      <td>TIER Mobility</td>\n      <td>Amsterdam, North Holland, Netherlands</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>THIS IS US\nTIER Mobility is Euro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://datai.jobs/job/arrival-text1-saint-pet...</td>\n      <td>Senior Data Engineer</td>\n      <td>Arrival</td>\n      <td>Saint Petersburg Yasnaya Polyana</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>At Arrival, our team is creating ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1693</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Lead Data Engineer (H/F)</td>\n      <td>Voyage Privé</td>\n      <td>Aix-En-Provence</td>\n      <td>CDI</td>\n      <td>E-commerce, Loisirs, Luxe</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>Aventure entrepreneuriale lancée en France en ...</td>\n    </tr>\n    <tr>\n      <th>1694</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer Confirmé(e)</td>\n      <td>Linkvalue</td>\n      <td>Paris</td>\n      <td>CDI</td>\n      <td>IT / Digital, Logiciels</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>L’ambition de Romain et Thibault, les co-fonda...</td>\n    </tr>\n    <tr>\n      <th>1695</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer H/F</td>\n      <td>Inetum</td>\n      <td>Courbevoie</td>\n      <td>CDI</td>\n      <td>IT / Digital</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>Inetum est une ESN agile, une société de servi...</td>\n    </tr>\n    <tr>\n      <th>1696</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer - Plateforme Big Data</td>\n      <td>Assistance Publique - Hôpitaux de Paris - DSI</td>\n      <td>Paris</td>\n      <td>CDI</td>\n      <td>Big Data, Intelligence artificielle / Machine ...</td>\n      <td>Télétravail partiel possible</td>\n      <td>2022-01-25</td>\n      <td>L’ Assistance Publique - Hôpitaux de Paris (AP...</td>\n    </tr>\n    <tr>\n      <th>1697</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer Junior H/F - Data &amp; Analytics</td>\n      <td>BearingPoint France</td>\n      <td>Paris La Défense</td>\n      <td>CDI</td>\n      <td>Organisation / Management, Stratégie, Transfor...</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>BearingPoint  est un cabinet de conseil en man...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1698 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.drop('id', axis=1, inplace=True)\n",
    "jobs = jobs.convert_dtypes()\n",
    "jobs['created_at'] = pd.to_datetime(jobs['created_at'])\n",
    "jobs['remote'].replace('N', 'Inconnu', inplace=True)\n",
    "\n",
    "jobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Titles"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing gender and special characters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [
    {
     "data": {
      "text/plain": "Data Engineer                                                                200\nSenior Data Engineer                                                          53\nLead Data Engineer                                                            14\nData Scientist                                                                13\nData Engineer (Remote)                                                        12\n                                                                            ... \nPréparateur(trice)  Superviseur(euse) Travaux Electricité Instrumentation      1\nSuperviseur(se) de Chantier Tuyauterie                                         1\nSenior / Lead Machine Learning Engineer                                        1\nConsultant Data Analytics & Engineer (CDI)                                     1\nData Engineer (/X)                                                             1\nName: title, Length: 1021, dtype: int64"
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genders = ['(H/F)', 'H/F', 'H / F'\n",
    "           '(F/H)', 'F/H',\n",
    "           '(M/F)', 'M/F',\n",
    "           '(F/M)', 'F/M',\n",
    "           'M/W', '(HF)'\n",
    "           '(M/F/D)', '(F/H/X)', '(m/f/d)', '(m/w/d)', '(H/S/T']\n",
    "\n",
    "def preprocess_title(title):\n",
    "    # Remove genders\n",
    "    for gender in genders:\n",
    "        title = title.replace(gender, '')\n",
    "    # Remove empty parenthesis\n",
    "    title = re.sub('\\([\\s]?\\)', '', title)\n",
    "    # Remove - |\n",
    "    title = title.replace('-', '')\n",
    "    title = title.replace('|', '')\n",
    "    title = title.replace('#', '')\n",
    "    # Remove whitespaces\n",
    "    title = title.strip()\n",
    "    return title\n",
    "\n",
    "jobs['title'] = jobs['title'].apply(lambda x: preprocess_title(x))\n",
    "jobs['title'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removings jobs which title's contains certain words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    url  \\\n0     https://datai.jobs/job/lyft-data-engineer-kyiv...   \n1     https://datai.jobs/job/chargepoint-data-engine...   \n2     https://datai.jobs/job/spotify-data-engineer-e...   \n3     https://datai.jobs/job/spotify-staff-data-engi...   \n4     https://datai.jobs/job/spotify-data-engineer-s...   \n...                                                 ...   \n1069  https://www.welcometothejungle.com/fr/companie...   \n1070  https://www.welcometothejungle.com/fr/companie...   \n1071  https://www.welcometothejungle.com/fr/companie...   \n1072  https://www.welcometothejungle.com/fr/companie...   \n1073  https://www.welcometothejungle.com/fr/companie...   \n\n                                        title  \\\n0                        Data Engineer – Kyiv   \n1                               Data Engineer   \n2                  Data Engineer – Experience   \n3            Staff Data Engineer – Experience   \n4                               Data Engineer   \n...                                       ...   \n1069          Data engineer  business centric   \n1070                Data Engineer Confirmé(e)   \n1071                            Data Engineer   \n1072       Data Engineer  Plateforme Big Data   \n1073  Data Engineer Junior   Data & Analytics   \n\n                                             company          location  \\\n0                                               Lyft     Kyiv, Ukraine   \n1                                        ChargePoint         Amsterdam   \n2                                            Spotify         Stockholm   \n3                                            Spotify         Stockholm   \n4                                            Spotify         Stockholm   \n...                                              ...               ...   \n1069                                          Heetch             Paris   \n1070                                       Linkvalue             Paris   \n1071                                          Inetum        Courbevoie   \n1072  Assistance Publique - Hôpitaux de Paris - DSI              Paris   \n1073                             BearingPoint France  Paris La Défense   \n\n           type                                           industry  \\\n0     Full Time                     Vehicles & Autonomous Mobility   \n1     Full Time                     Vehicles & Autonomous Mobility   \n2     Full Time                                      Entertainment   \n3     Full Time                                      Entertainment   \n4     Full Time                                      Entertainment   \n...         ...                                                ...   \n1069        CDI                              Mobile Apps, Mobility   \n1070        CDI                            IT / Digital, Logiciels   \n1071        CDI                                       IT / Digital   \n1072        CDI  Big Data, Intelligence artificielle / Machine ...   \n1073        CDI  Organisation / Management, Stratégie, Transfor...   \n\n                            remote created_at  \\\n0                          Inconnu 2021-12-27   \n1                          Inconnu 2021-12-27   \n2                          Inconnu 2021-12-27   \n3                          Inconnu 2021-12-27   \n4                          Inconnu 2021-12-27   \n...                            ...        ...   \n1069                       Inconnu 2022-01-25   \n1070                       Inconnu 2022-01-25   \n1071                       Inconnu 2022-01-25   \n1072  Télétravail partiel possible 2022-01-25   \n1073                       Inconnu 2022-01-25   \n\n                                                   text  \n0     \n            At Lyft, our mission is to improv...  \n1     \n            Data Engineer\nAbout Us\nWith elect...  \n2     \n            Delivering the best Spotify exper...  \n3     \n            Delivering the best Spotify exper...  \n4     \n            The Platform team creates the tec...  \n...                                                 ...  \n1069  Heetch: the VTC that keeps everyone moving. Ou...  \n1070  L’ambition de Romain et Thibault, les co-fonda...  \n1071  Inetum est une ESN agile, une société de servi...  \n1072  L’ Assistance Publique - Hôpitaux de Paris (AP...  \n1073  BearingPoint  est un cabinet de conseil en man...  \n\n[1074 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>title</th>\n      <th>company</th>\n      <th>location</th>\n      <th>type</th>\n      <th>industry</th>\n      <th>remote</th>\n      <th>created_at</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://datai.jobs/job/lyft-data-engineer-kyiv...</td>\n      <td>Data Engineer – Kyiv</td>\n      <td>Lyft</td>\n      <td>Kyiv, Ukraine</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>At Lyft, our mission is to improv...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://datai.jobs/job/chargepoint-data-engine...</td>\n      <td>Data Engineer</td>\n      <td>ChargePoint</td>\n      <td>Amsterdam</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>Data Engineer\nAbout Us\nWith elect...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://datai.jobs/job/spotify-data-engineer-e...</td>\n      <td>Data Engineer – Experience</td>\n      <td>Spotify</td>\n      <td>Stockholm</td>\n      <td>Full Time</td>\n      <td>Entertainment</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>Delivering the best Spotify exper...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://datai.jobs/job/spotify-staff-data-engi...</td>\n      <td>Staff Data Engineer – Experience</td>\n      <td>Spotify</td>\n      <td>Stockholm</td>\n      <td>Full Time</td>\n      <td>Entertainment</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>Delivering the best Spotify exper...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://datai.jobs/job/spotify-data-engineer-s...</td>\n      <td>Data Engineer</td>\n      <td>Spotify</td>\n      <td>Stockholm</td>\n      <td>Full Time</td>\n      <td>Entertainment</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>The Platform team creates the tec...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data engineer  business centric</td>\n      <td>Heetch</td>\n      <td>Paris</td>\n      <td>CDI</td>\n      <td>Mobile Apps, Mobility</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>Heetch: the VTC that keeps everyone moving. Ou...</td>\n    </tr>\n    <tr>\n      <th>1070</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer Confirmé(e)</td>\n      <td>Linkvalue</td>\n      <td>Paris</td>\n      <td>CDI</td>\n      <td>IT / Digital, Logiciels</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>L’ambition de Romain et Thibault, les co-fonda...</td>\n    </tr>\n    <tr>\n      <th>1071</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer</td>\n      <td>Inetum</td>\n      <td>Courbevoie</td>\n      <td>CDI</td>\n      <td>IT / Digital</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>Inetum est une ESN agile, une société de servi...</td>\n    </tr>\n    <tr>\n      <th>1072</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer  Plateforme Big Data</td>\n      <td>Assistance Publique - Hôpitaux de Paris - DSI</td>\n      <td>Paris</td>\n      <td>CDI</td>\n      <td>Big Data, Intelligence artificielle / Machine ...</td>\n      <td>Télétravail partiel possible</td>\n      <td>2022-01-25</td>\n      <td>L’ Assistance Publique - Hôpitaux de Paris (AP...</td>\n    </tr>\n    <tr>\n      <th>1073</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer Junior   Data &amp; Analytics</td>\n      <td>BearingPoint France</td>\n      <td>Paris La Défense</td>\n      <td>CDI</td>\n      <td>Organisation / Management, Stratégie, Transfor...</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>BearingPoint  est un cabinet de conseil en man...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1074 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unwanted = ['Lead', 'Senior', 'Manager', 'Software Engineer', 'Head', 'Backend', 'Android']\n",
    "unsure = ['Architecte', 'Architect']\n",
    "excluded_words = unwanted + unsure\n",
    "\n",
    "for word in excluded_words:\n",
    "    jobs.drop(jobs[jobs['title'].str.contains(word)].index, inplace=True)\n",
    "\n",
    "jobs.reset_index(drop=True, inplace=True)\n",
    "jobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Language"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "outputs": [],
   "source": [
    "jobs['lang'] = jobs['text'].apply(lambda x: detect(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lang'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/Reviews/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3620\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/PycharmProjects/Reviews/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/Reviews/venv/lib/python3.9/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 'lang'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[0;32mIn [388]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mjobs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlang\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mvalue_counts()\n",
      "File \u001B[0;32m~/PycharmProjects/Reviews/venv/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3503\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   3504\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3505\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3507\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/PycharmProjects/Reviews/venv/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3621\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine\u001B[38;5;241m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3622\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m-> 3623\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   3624\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m   3625\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3626\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3627\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3628\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: 'lang'"
     ]
    }
   ],
   "source": [
    "jobs['lang'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "jobs.drop(jobs[jobs['lang'] == 'cs'].index, axis=0, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "def remove_excessive_spaces(text):\n",
    "    # Remove newline at beginning of text\n",
    "    text = re.sub('^[\\\\]n[\\s]*', '', text)\n",
    "    # Remove \\xa0 (non-breaking space in Latin1 ISO 8859-1)\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    # Remove newlines if there is at least 2 consecutive\n",
    "    text = re.sub('(\\\\n\\s\\\\n)', '\\n', text)\n",
    "    text = re.sub('(\\\\n\\\\n)', '\\n', text)\n",
    "    text = re.sub('(\\\\n\\\\n\\\\n)', '\\n', text)\n",
    "    text = re.sub('(\\\\n\\\\n\\\\n\\\\n)', '\\n', text)\n",
    "    text = re.sub('(\\\\n\\\\n\\\\n\\\\n\\\\n)', '\\n', text)\n",
    "    return text\n",
    "\n",
    "jobs.text = jobs['text'].apply(lambda x: remove_excessive_spaces(x))\n",
    "jobs.text = jobs['text'].apply(lambda x: remove_excessive_spaces(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Name Entity Recognition of technologies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Manual annotation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "english_collective_dict = {'TRAINING_DATA': []}\n",
    "french_collective_dict = {'TRAINING_DATA': []}\n",
    "\n",
    "def structure_training_data(text, kw_list, collective_dict):\n",
    "    results = []\n",
    "    entities = []\n",
    "\n",
    "    for kw in tqdm(kw_list):\n",
    "        search_ = re.finditer(kw, text, flags=re.IGNORECASE)\n",
    "\n",
    "        matches_positions = [[m.start(), m.end()] for m in search_]\n",
    "\n",
    "        if len(matches_positions) > 0:\n",
    "            for match_positions in matches_positions:\n",
    "                start = match_positions[0]\n",
    "                end = match_positions[1]\n",
    "                entities.append((start, end, \"TECHNO\"))\n",
    "        else:\n",
    "            print(\"No pattern matches found. Keyword: \", kw)\n",
    "\n",
    "    if len(entities) > 0:\n",
    "        results = [text, {'entities': entities}]\n",
    "        collective_dict['TRAINING_DATA'].append(results)\n",
    "        return"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. English data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [
    {
     "data": {
      "text/plain": "[0,\n 1,\n 2,\n 3,\n 4,\n 5,\n 6,\n 7,\n 8,\n 9,\n 10,\n 11,\n 12,\n 13,\n 14,\n 15,\n 16,\n 17,\n 18,\n 19,\n 20,\n 21,\n 22,\n 23,\n 24,\n 25,\n 26,\n 27,\n 28,\n 29,\n 30,\n 31,\n 32,\n 33,\n 34,\n 35,\n 36,\n 37,\n 38,\n 39,\n 40,\n 41,\n 42,\n 43,\n 44,\n 45,\n 46,\n 47,\n 48,\n 49,\n 50,\n 51,\n 52,\n 53,\n 54,\n 55,\n 56,\n 57,\n 58,\n 59,\n 60,\n 61,\n 62,\n 63,\n 64,\n 65,\n 66,\n 67,\n 68,\n 69,\n 70,\n 71,\n 72,\n 73,\n 74,\n 75,\n 76,\n 77,\n 78,\n 79,\n 80,\n 81,\n 82,\n 83,\n 87,\n 92,\n 93,\n 94,\n 97,\n 98,\n 100,\n 105,\n 106,\n 107,\n 114,\n 115,\n 117,\n 118,\n 120,\n 121,\n 122,\n 125,\n 129,\n 132,\n 133,\n 136,\n 138,\n 139,\n 141,\n 142,\n 143,\n 146,\n 148,\n 149,\n 150,\n 151,\n 154,\n 155,\n 158,\n 160,\n 162,\n 164,\n 167,\n 168,\n 169,\n 172,\n 175,\n 177,\n 179,\n 182,\n 184,\n 186,\n 193,\n 197,\n 200,\n 201,\n 202,\n 204,\n 208,\n 214,\n 217,\n 218,\n 219,\n 225,\n 228,\n 232,\n 235,\n 237,\n 238,\n 241,\n 250,\n 252,\n 257,\n 260,\n 262,\n 263,\n 272,\n 277,\n 278,\n 280,\n 283,\n 284,\n 285,\n 286,\n 287,\n 288,\n 289,\n 290,\n 292,\n 295,\n 296,\n 299,\n 301,\n 304,\n 305,\n 309,\n 318,\n 320,\n 328,\n 341,\n 352,\n 354,\n 355,\n 357,\n 362,\n 365,\n 367,\n 368,\n 372,\n 376,\n 378,\n 379,\n 380,\n 385,\n 387,\n 393,\n 396,\n 398,\n 399,\n 400,\n 408,\n 411,\n 412,\n 414,\n 415,\n 419,\n 420,\n 422,\n 426,\n 427,\n 433,\n 435,\n 442,\n 444,\n 446,\n 451,\n 454,\n 455,\n 456,\n 460,\n 461,\n 464,\n 467,\n 470,\n 471,\n 473,\n 474,\n 477,\n 478,\n 479,\n 480,\n 481,\n 485,\n 491,\n 495,\n 496,\n 497,\n 499,\n 500,\n 501,\n 503,\n 504,\n 507,\n 509,\n 510,\n 511,\n 512,\n 516,\n 518,\n 519,\n 521,\n 523,\n 524,\n 526,\n 529,\n 532,\n 533,\n 534,\n 535,\n 536,\n 537,\n 539,\n 540,\n 541,\n 543,\n 544,\n 545,\n 546,\n 548,\n 549,\n 550,\n 552,\n 556,\n 558,\n 560,\n 564,\n 565,\n 566,\n 567,\n 570,\n 571,\n 572,\n 575,\n 576,\n 577,\n 578,\n 583,\n 584,\n 587,\n 588,\n 590,\n 593,\n 595,\n 596,\n 597,\n 599,\n 602,\n 603,\n 604,\n 605,\n 607,\n 609,\n 610,\n 611,\n 612,\n 613,\n 614,\n 615,\n 619,\n 622,\n 623,\n 624,\n 625,\n 626,\n 631,\n 633,\n 636,\n 637,\n 638,\n 639,\n 640,\n 641,\n 644,\n 646,\n 648,\n 649,\n 650,\n 651,\n 652,\n 653,\n 655,\n 657,\n 658,\n 661,\n 662,\n 663,\n 664,\n 665,\n 667,\n 668,\n 670,\n 671,\n 672,\n 673,\n 674,\n 675,\n 676,\n 677,\n 678,\n 681,\n 682,\n 683,\n 687,\n 690,\n 691,\n 692,\n 694,\n 696,\n 697,\n 698,\n 699,\n 700,\n 701,\n 703,\n 704,\n 705,\n 706,\n 707,\n 709,\n 711,\n 712,\n 714,\n 716,\n 720,\n 721,\n 724,\n 725,\n 726,\n 728,\n 729,\n 734,\n 735,\n 740,\n 741,\n 744,\n 746,\n 751,\n 752,\n 753,\n 757,\n 761,\n 765,\n 766,\n 767,\n 768,\n 770,\n 771,\n 772,\n 773,\n 774,\n 775,\n 776,\n 777,\n 779,\n 780,\n 785,\n 786,\n 787,\n 788,\n 792,\n 795,\n 797,\n 798,\n 799,\n 800,\n 801,\n 802,\n 805,\n 806,\n 811,\n 814,\n 817,\n 818,\n 819,\n 820,\n 821,\n 826,\n 827,\n 828,\n 832,\n 833,\n 834,\n 845,\n 850,\n 855,\n 860,\n 861,\n 866,\n 870,\n 871,\n 875,\n 878,\n 881,\n 882,\n 883,\n 884,\n 885,\n 886,\n 887,\n 888,\n 889,\n 890,\n 892,\n 894,\n 895,\n 897,\n 901,\n 907,\n 908,\n 909,\n 914,\n 916,\n 920,\n 921,\n 924,\n 925,\n 926,\n 930,\n 932,\n 933,\n 935,\n 939,\n 941,\n 952,\n 954,\n 957,\n 960,\n 962,\n 965,\n 966,\n 967,\n 968,\n 969,\n 973,\n 977,\n 978,\n 980,\n 984,\n 985,\n 986,\n 989,\n 990,\n 992,\n 993,\n 994,\n 995,\n 999,\n 1001,\n 1004,\n 1008,\n 1011,\n 1015,\n 1017,\n 1023,\n 1026,\n 1028,\n 1029,\n 1032,\n 1036,\n 1037,\n 1038,\n 1039,\n 1040,\n 1041,\n 1042,\n 1043,\n 1048,\n 1049,\n 1050,\n 1057,\n 1064,\n 1065,\n 1069]"
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_jobs = jobs[jobs['lang'] == 'en']\n",
    "list(english_jobs.index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "i = 549\n",
    "print(english_jobs.loc[i, 'title'].upper(), '\\n')\n",
    "print(english_jobs.text[i])\n",
    "\n",
    "# structure_training_data(jobs.text[], [''], english_collective_dict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 311,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA ENGINEER \n",
      "\n",
      "Homa Games is the world’s leading platform for creating and publishing mobile games.  Game developers from around the world use Homa’s technologies to power their creativity. We’re a team of 100 creative, data-centric minds helping developers to turn their games into global hits played by millions of users. Based in Paris, in the last year we opened 5 offices in 4 countries and doubled our team. Our strategy relies on powerful Tech built in-house.  Homa’s platform provides a comprehensive set of software solutions to scout new trends and ideas, execute real-time A/B testing of all the features of the game through a no code SDK, and to distribute and monetize the games. Through this platform, Homa provides an end-to-end digital ecosystem for developers to focus on the right idea, optimize all the features, and distribute the games worldwide massively. Apps powered by Homa creators have been downloaded more than 500 million times. We are a diverse team from various backgrounds  (artists, business, engineers, entrepreneurs, ex strategy consultants) and origins (>20 nationalities). We are passionate about games and working with studios & platform partners all around the world (US, Europe, and Asia).  We aim to give our games a step ahead of competition, tailor-made support to our studio partners, and fair & transparent cooperation. In October 2021,  we raised a $50 million  Series A round led by Northzone with participation of Singular and renowned business angels including King and FuboTV founders and Spotify founder Daniel Ek’s family office, on top of a previous $15 million round with seed investors Headline and Eurazeo in February. At Homa, Data is at the heart of our platform as it enables us to  Scan the market for new trends and ideas so that game developers can work on the next big hit\n",
      " Optimize User Acquisition strategies so that we can bring in more people playing our games\n",
      " Optimize Game Development so that players enjoy our games\n",
      " The Data Engineering team missions includes : Creating and maintaining our Data Platform which Extracts and Load data and then verify and transform our data to expose our Core Data Model to other teams\n",
      " Automation : we provide tools to allow other teams to create their own data and tools on top of our Core Data Model\n",
      " We are impact oriented and focus on projects that will improve the efficiency of Homa through the access to data. Within this context we are looking for a Data Engineer to join our team Our data stack contains: AWS\n",
      " Python\n",
      " Airflow + k8s\n",
      " dbt\n",
      " Redshift\n",
      " Tableau\n",
      " Your mission will be to Create and maintain data pipelines\n",
      " Acquire data : connect to new data sources, maintain and scale existing pipelines\n",
      " Model data and make it accessible : team up with Analytics team and Ops teams to build and maintain datamarts\n",
      " Who you are Humble and impact oriented, you are autonomous and communicate, a lot. You embrace the done is better than perfect motto.\n",
      " Good knowledge of Python for data pipelines\n",
      " Experience with a cloud infrastructure (AWS is prefered)\n",
      " Experience with dbt and strong SQL skills\n",
      " Familiar with the best pratices to work with a team: versioning, code review, continuous deployment, automated testing\n",
      " This content is blocked Youtube cookies are required to show you this content Accept cookies\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 8978.42it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 11201.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pattern matches found. Keyword:  (No)SQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 8756.38it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 8709.91it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 9524.18it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 7677.86it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 8293.82it/s]\n",
      "100%|██████████| 17/17 [00:00<00:00, 9290.32it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 12817.91it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 7881.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pattern matches found. Keyword:  Matillion WTL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 10770.41it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 15547.26it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 5485.82it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 10018.24it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 9530.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pattern matches found. Keyword:  Reddis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 13678.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 6092.83it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 10582.13it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 8454.71it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 5804.69it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 9249.67it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 7950.45it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 9152.61it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 4401.16it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 13781.94it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 11115.65it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 8680.26it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 8198.00it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 8131.65it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 8860.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 8502.54it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 9642.08it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 12520.31it/s]\n",
      "100%|██████████| 14/14 [00:00<00:00, 15913.35it/s]\n",
      "100%|██████████| 12/12 [00:00<00:00, 13714.35it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 12943.28it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 14160.98it/s]\n",
      "100%|██████████| 6/6 [00:00<00:00, 10695.21it/s]\n"
     ]
    }
   ],
   "source": [
    "structure_training_data(english_jobs.text[0], ['ETL', 'Hive', 'Airflow', 'Flyte', 'SQL', 'S3', 'DynamoDB', 'Kafka', 'ElasticSearch', 'Spark', 'Flyte', 'Stackdriver', 'SageMaker'], english_collective_dict)\n",
    "structure_training_data(english_jobs.text[1], ['airflow', 'mlflow', 'Kubernetes', '(No)SQL', 'Airflow', 'Python', 'nodejs'], english_collective_dict)\n",
    "structure_training_data(english_jobs.text[2], ['BigQuery', 'Dataflow', 'Pub/Sub', 'Apache Beam', 'Tensorflow', 'Kubeflow'], english_collective_dict)\n",
    "structure_training_data(english_jobs.text[5], ['Spark', 'Luigi', 'Kafka', 'Scala', 'Java', 'Python', 'Go', 'Spark', 'Hadoop'], english_collective_dict)\n",
    "structure_training_data(english_jobs.text[9], ['SQL', 'Snowflake', 'AWS', 'Google Cloud', 'Microsoft Azure', 'Talend', 'Airflow', 'dbt', 'Looker', 'Apache Airflow', 'SQL server', 'Python', 'Fivetran'], english_collective_dict)\n",
    "structure_training_data(jobs.text[10], ['MySQL', 'Spanner', 'CloudSQL', 'PHP', 'Golang', 'Python', 'Bash',], english_collective_dict)\n",
    "structure_training_data(jobs.text[11], ['MongoDB', 'MySQL', 'Go', 'Ruby', 'Java', 'Golang', 'Vitess'], english_collective_dict)\n",
    "structure_training_data(jobs.text[12], ['PostgreSQL', 'Redis', 'Quicksilver', 'Ceph', 'HDFS', 'Go', 'Python', 'C/C\\+\\+', 'Prometheus', 'Grafana', 'Kibana', 'Salt', 'CockroachDB', 'OpenTSDB', 'ClickHouse', 'ElasticSearch', 'SQL'], english_collective_dict)\n",
    "structure_training_data(jobs.text[15], ['Hadoop', 'Spark', 'Pig', 'R', 'Python', 'SQL', 'Scala', 'Javascript', 'React'], english_collective_dict)\n",
    "structure_training_data(jobs.text[18], ['SQL', 'AWS Redshift', 'Matillion WTL', 'Astronomer', 'Airflow', 'Kafka', 'NiFi', 'Python', 'dbt', 'AWS', 'AWS Glue', 'Redshift Spectrum', 'AWS S3', 'Looker'], english_collective_dict)\n",
    "structure_training_data(jobs.text[24], ['SAP', 'SQL', 'Tableau', 'PowerBI', 'VizQL', 'DAX', 'PQL'], english_collective_dict)\n",
    "structure_training_data(jobs.text[36], ['Python', 'Scala', 'Java', 'Scikit Learn', 'TensorFlow', 'H20', 'Py torch', 'Linux', 'Unix'], english_collective_dict)\n",
    "structure_training_data(jobs.text[37], ['Python', 'Scala', 'Pandas', 'R', 'BigQuery', 'SQL', 'Tableau'], english_collective_dict)\n",
    "structure_training_data(jobs.text[100], ['Airflow', 'Google Cloud Platform', 'BigQuery', 'Beam', 'Dataflow', 'Flink', 'Docker', 'K8S', 'Jenkins', 'SQL', 'Python', 'Go lang', 'Java', 'Kubernetes', 'Datadog'], english_collective_dict)\n",
    "structure_training_data(jobs.text[105], ['GraphQL', 'React', 'Apollo', 'Node', 'Typescript', 'Postgres', 'Reddis', 'RabbitMQ', 'Neo4J', 'Github', 'CircleCI', 'Codecov', 'AWS'], english_collective_dict)\n",
    "structure_training_data(jobs.text[106], ['Python', 'Java', 'HTTP', 'SQL', 'NoSQL', 'Hadoop', 'Spark', 'BigQuery'], english_collective_dict)\n",
    "structure_training_data(jobs.text[107], ['Airbyte', 'StitchData', 'Fivetran', 'BigQuery', 'Snowflake', 'Airflow', 'DataBuildTool', 'Metabase', 'SQL', 'Python'], english_collective_dict)\n",
    "structure_training_data(jobs.text[114], ['AWS', 'S3', 'EMR', 'Glue', 'Kinesis', 'Athena', 'Python', 'Scala', 'Java', 'Spark', 'Flink', 'MapReduce', 'Hadoop', 'SQL'], english_collective_dict)\n",
    "structure_training_data(jobs.text[125], ['BigQuery', 'dbt', 'airflow', 'Microstrategy', 'SQL', 'Snowflake', 'Kimball', 'data vault', 'git', 'Pagerduty', 'python'], english_collective_dict)\n",
    "structure_training_data(jobs.text[129], ['PubSub', 'python', 'SQL', 'airflow', 'Luigi', 'GCP', 'Kafka'], english_collective_dict)\n",
    "structure_training_data(jobs.text[138], ['Spark', 'Hadoop', 'Kafka', 'Cassandra', 'Mongo', 'Python', 'SQL', 'Scala', 'Azure', 'AWS', 'GCP'], english_collective_dict)\n",
    "structure_training_data(jobs.text[139], ['Python', 'SQL', 'BigQuery', 'SQL Server', 'Postgres', 'Git', 'GCP', 'Azure', 'AWS'], english_collective_dict)\n",
    "structure_training_data(jobs.text[149], ['SQL', 'MySQL', 'Oracle', 'Snowflake', 'Microsoft SSIS', 'Matillion', 'Talend', 'Informatica', 'Python', 'Scala', 'Java', 'PHP', 'Unix Shell', 'Hadoop', 'SPAR', 'MAPR', 'PowerBI', 'Tableau', 'Qlikview'], english_collective_dict)\n",
    "structure_training_data(jobs.text[158], ['SQL'], english_collective_dict)\n",
    "structure_training_data(jobs.text[160], ['Java', 'Scala', 'Python', 'Spark', 'BigQuery', 'DataStudio', 'Cassandra', 'ClickHouse', 'MySQL', 'GCP', 'Kubernetes', 'Grafana'], english_collective_dict)\n",
    "structure_training_data(jobs.text[175], ['Python', 'PostgreSQL', 'RabbitMQ'], english_collective_dict)\n",
    "structure_training_data(jobs.text[521], ['SQL', 'Spark', 'Kubernetes', 'AWS', 'Azure', 'GCP', 'Snowflake', 'BigQuery', 'Redshift', 'Synapse', 'PostgreSQL', 'MongoDB', 'Java', 'Python', 'Javascript'], english_collective_dict)\n",
    "structure_training_data(jobs.text[544], ['GCP', 'Tensorflow', 'PyTorch', 'MxNet', 'Python', 'Numpy', 'Scipy', 'Pandas'], english_collective_dict)\n",
    "structure_training_data(jobs.text[548], ['Docker', 'IAM', 'GCP', 'Git', 'Gitlab', 'Linux', 'Docker', 'Kubernetes', 'LAMP', 'Python'], english_collective_dict)\n",
    "structure_training_data(jobs.text[549], ['AWS', 'Airflow', 'k8s', 'dbt', 'Redshift', 'Tableau', 'Python', 'SQL'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1042], ['Scala', 'Go', 'Kafka', 'Spark', 'Akka', 'ClickHouse', 'Elasticsearch', 'Java', 'Go', 'AWS'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1048], ['SQL', 'ElasticSearch', 'C#'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1049], ['AWS', 'Redshift', 'UNIX', 'Shell', 'Python', 'Perl', 'Ruby'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1050], ['S3', 'EMR', 'EC2', 'Glue', 'Kinesis', 'Athena', 'Python', 'Scala', 'Java', 'Spark', 'Flink', 'MapReduce', 'Hadoop', 'SQL'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1057], ['Python', 'Go', 'Spark', 'Kafka', 'Snowflake', 'NoSQL', 'Git', 'Linux', 'Django', 'Celery', 'RabbitMQ', 'ElasticSearch'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1064], ['Kubernetes', 'BigQuery', 'Apache Beam', 'Apache Airflow', 'Java', 'Python', 'Protobuf', 'gRPC', 'Kafka', 'GCP', 'Istio', 'Grafana', 'Prometheus', 'Git', 'MS-SQL', 'PostgreSQL', 'Bigtable', 'HBase', 'ElasticSearch'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1065], ['Python', 'Airflow', 'Docker', 'Apache Kafka', 'Tensorflow', 'AWS', 'S3', 'Redshift', 'Athena', 'Hive', 'SparkSQL', 'Java', 'Scala', 'Hadoop', 'Spark', 'Flink'], english_collective_dict)\n",
    "structure_training_data(jobs.text[1069], ['s3', 'Redshift', 'dataiku', 'airflow', 'SQL', 'Python'], english_collective_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. French data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "french_jobs = jobs[jobs['lang'] == 'fr']\n",
    "french_jobs.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "french_jobs.index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "i = 84\n",
    "print(french_jobs.loc[i, 'title'].upper(), '\\n')\n",
    "print(french_jobs.text[i])\n",
    "\n",
    "structure_training_data(jobs.text[], [])\n",
    "structure_training_data(jobs.text[1042], [])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting training examples into spaCy Doc objects"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [],
   "source": [
    "def create_training(train_data):\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(train_data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "\n",
    "        for start, end, label in annot['entities']:\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode='contract')\n",
    "\n",
    "            if span is None:\n",
    "                print('Skipping entity.')\n",
    "            else:\n",
    "                ents.append(span)\n",
    "                try:\n",
    "                    doc.ents = ents\n",
    "                except:\n",
    "                    ents.pop()\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    return db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. English"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [
    {
     "data": {
      "text/plain": "19.0"
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_collective_dict['TRAINING_DATA']) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 12/19 [00:00<00:00, 53.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 51.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 8/19 [00:00<00:00, 71.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 16/19 [00:00<00:00, 64.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity.\n",
      "Skipping entity."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 63.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank('en')\n",
    "\n",
    "english_train_data = english_collective_dict['TRAINING_DATA'][:19]\n",
    "english_evaluation_data = english_collective_dict['TRAINING_DATA'][19:]\n",
    "\n",
    "TRAIN_DATA_DOC = create_training(english_train_data)\n",
    "TRAIN_DATA_DOC.to_disk('./train_data/ENGLISH_TRAIN_DATA.spacy')\n",
    "\n",
    "VALID_DATA_DOC = create_training(english_evaluation_data)\n",
    "VALID_DATA_DOC.to_disk('./train_data/ENGLISH_VALID_DATA.spacy')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1. English"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python3 -m spacy init fill-config base_config_english.cfg english_config.cfg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 316,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m✔ Auto-filled config with all values\u001B[0m\r\n",
      "\u001B[38;5;2m✔ Saved config\u001B[0m\r\n",
      "english_config.cfg\r\n",
      "You can now add your data and train your pipeline:\r\n",
      "python -m spacy train english_config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Saving to output directory: output_english\u001B[0m\r\n",
      "\u001B[38;5;4mℹ Using CPU\u001B[0m\r\n",
      "\u001B[1m\r\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\r\n",
      "[2022-02-21 08:15:57,791] [INFO] Set up nlp object from config\r\n",
      "[2022-02-21 08:15:57,805] [INFO] Pipeline: ['tok2vec', 'ner']\r\n",
      "[2022-02-21 08:15:57,809] [INFO] Created vocabulary\r\n",
      "[2022-02-21 08:15:57,810] [INFO] Finished initializing nlp object\r\n",
      "[2022-02-21 08:15:59,462] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\r\n",
      "\u001B[38;5;2m✔ Initialized pipeline\u001B[0m\r\n",
      "\u001B[1m\r\n",
      "============================= Training pipeline =============================\u001B[0m\r\n",
      "\u001B[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001B[0m\r\n",
      "\u001B[38;5;4mℹ Initial learn rate: 0.001\u001B[0m\r\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \r\n",
      "---  ------  ------------  --------  ------  ------  ------  ------\r\n",
      "  0       0          0.00    490.83    0.00    0.00    0.00    0.00\r\n",
      " 10     200        183.69   5798.60   73.56   78.87   68.92    0.74\r\n",
      " 21     400         49.23     47.23   74.94   76.53   73.42    0.75\r\n",
      " 31     600         17.88      9.15   75.99   78.74   73.42    0.76\r\n",
      " 42     800         37.91     22.31   72.38   68.98   76.13    0.72\r\n",
      " 52    1000         24.01     11.14   74.59   77.29   72.07    0.75\r\n",
      " 63    1200         38.49     14.61   74.04   74.21   73.87    0.74\r\n",
      " 73    1400          4.10      1.99   76.48   80.90   72.52    0.76\r\n",
      " 84    1600         43.94     18.66   75.59   78.92   72.52    0.76\r\n",
      " 94    1800         71.08     32.12   70.51   67.07   74.32    0.71\r\n",
      "105    2000         18.37      7.83   73.99   73.66   74.32    0.74\r\n",
      "115    2200          7.48      2.90   75.23   78.16   72.52    0.75\r\n",
      "126    2400         68.73     22.20   76.67   81.31   72.52    0.77\r\n",
      "136    2600        113.96     40.34   73.06   74.07   72.07    0.73\r\n",
      "147    2800         81.93     29.76   76.92   82.47   72.07    0.77\r\n",
      "157    3000         26.40      9.59   75.06   79.40   71.17    0.75\r\n",
      "168    3200         28.26      9.97   72.22   82.18   64.41    0.72\r\n",
      "178    3400          0.19      0.06   70.78   71.76   69.82    0.71\r\n",
      "189    3600          0.00      0.00   73.11   76.73   69.82    0.73\r\n",
      "200    3800          0.00      0.00   72.81   76.62   69.37    0.73\r\n",
      "210    4000          0.00      0.00   71.79   74.40   69.37    0.72\r\n",
      "221    4200          0.00      0.00   71.79   74.40   69.37    0.72\r\n",
      "231    4400          0.00      0.00   71.79   74.40   69.37    0.72\r\n",
      "\u001B[38;5;2m✔ Saved pipeline to output directory\u001B[0m\r\n",
      "output_english/model-last\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train english_config.cfg --output ./output_english"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. French"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "!python3 -m spacy init fill-config base_config_french.cfg french_config.cfg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "!python3 -m spacy train french_config.cfg --output ./output_french"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Founded in 2017, \n<mark class=\"entity\" style=\"background: linear-gradient(90deg, #E1D436, #F59710); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    OppScience\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TECHNO</span>\n</mark>\n is an upcoming player in the European Big Data scene. OppScience created the Bee4sense platform which allows the integration of large volumes of structured and unstructured data. Our goal : Raise Bee4sens platform as main French solution for processing heterogeneous data. By joining us, you will integrate a dynamic and passionate structure in which you can improve your skills by working on latest technologies in a booming sector. Join us ! We are looking for a technical lead consultant, member of the customer facing team, to manage the implementation of our bee4sense platform and adapt it to the specific needs of customers through configurations and lightweight developments. You will be in charge of the projects implementation of our platform by defining its architecture and managing internal and partner resources. In close collaboration with the Customer Success Directors and R&amp;D, you are responsible for defining the best application architecture and managing the implementation resources. You guarantee an optimal implementation by ensuring on-time delivery. You are also a key support for our partners in their own implementations. Your responsibilities are as follows: By following an agile methodology, you : – Collect customer requirements in close collaboration with the Customer Success Director, – Define the architecture of the deployments in relation with R&amp;D, – Manage internal resources. – You are engineer – You have 2 or more years of experience in \n<mark class=\"entity\" style=\"background: linear-gradient(90deg, #E1D436, #F59710); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    NoSQL\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TECHNO</span>\n</mark>\n technologies – You are curious and have a good sense of suggestion Required skills – You are familiar with agile methodologies – You have a strong ability to understand customer needs – You have a strong sense of initiative – You master CSS, HTML, Jquery, AngularJS et Angular – You with strong abilities to understand complex architectures – You learn very quickly – You speak French and English fluently – You have the entrepreneurial spirit – You are comfortable with data: GED, CRM, NLP, AI, search engine, DBMS – You know Git, Maven and the Atlassian Suite Phone call interview Technical tests Physical interview with the CEO This content is blocked Youtube cookies are required to show you this content Accept cookies</div></span>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp_english_output = spacy.load('/Users/donor/PycharmProjects/DE_job_market/nlp/output_english/model-best')\n",
    "\n",
    "doc = nlp_english_output(english_jobs.text[716])\n",
    "colors = {\"TECHNO\": \"linear-gradient(90deg, #E1D436, #F59710)\"}\n",
    "options = {\"ents\": [\"TECHNO\"], \"colors\": colors}\n",
    "displacy.render(doc, style='ent', options=options)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OppScience TECHNO\n",
      "NoSQL TECHNO\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extracting technologies into a new column"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [],
   "source": [
    "nlp_english_output = spacy.load('/Users/donor/PycharmProjects/DE_job_market/nlp/output_english/model-best')\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "def extract_technos(text):\n",
    "    doc = nlp_english_output(text)\n",
    "    technos = [ent.text for ent in doc.ents]\n",
    "    return list(set(technos))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/2w2qlxsn13s9qyf11rtgzkkr0000gn/T/ipykernel_45579/3063333733.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  english_jobs['technos'] = english_jobs['text'].apply(lambda x: extract_technos(x))\n"
     ]
    }
   ],
   "source": [
    "english_jobs['technos'] = english_jobs['text'].apply(lambda x: extract_technos(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    url  \\\n0     https://datai.jobs/job/lyft-data-engineer-kyiv...   \n1     https://datai.jobs/job/chargepoint-data-engine...   \n2     https://datai.jobs/job/spotify-data-engineer-e...   \n3     https://datai.jobs/job/spotify-staff-data-engi...   \n4     https://datai.jobs/job/spotify-data-engineer-s...   \n...                                                 ...   \n1050  https://www.welcometothejungle.com/fr/companie...   \n1057  https://www.welcometothejungle.com/fr/companie...   \n1064  https://www.welcometothejungle.com/fr/companie...   \n1065  https://www.welcometothejungle.com/fr/companie...   \n1069  https://www.welcometothejungle.com/fr/companie...   \n\n                                 title               company       location  \\\n0                 Data Engineer – Kyiv                  Lyft  Kyiv, Ukraine   \n1                        Data Engineer           ChargePoint      Amsterdam   \n2           Data Engineer – Experience               Spotify      Stockholm   \n3     Staff Data Engineer – Experience               Spotify      Stockholm   \n4                        Data Engineer               Spotify      Stockholm   \n...                                ...                   ...            ...   \n1050                     Data Engineer  GLOBAL SAVINGS GROUP        München   \n1057                     Data Engineer           Back Market       Bordeaux   \n1064     Data Engineer (Platform team)                Veepee          Paris   \n1065            Data Engineer (Remote)                Stuart              N   \n1069   Data engineer  business centric                Heetch          Paris   \n\n           type                                           industry  \\\n0     Full Time                     Vehicles & Autonomous Mobility   \n1     Full Time                     Vehicles & Autonomous Mobility   \n2     Full Time                                      Entertainment   \n3     Full Time                                      Entertainment   \n4     Full Time                                      Entertainment   \n...         ...                                                ...   \n1050        CDI         AdTech / MarTech, E-commerce, IT / Digital   \n1057        CDI  Collaborative Economy, E-commerce, Environment...   \n1064        CDI                                         E-commerce   \n1065        CDI                   Collaborative Economy, Logistics   \n1069        CDI                              Mobile Apps, Mobility   \n\n                            remote created_at  \\\n0                          Inconnu 2021-12-27   \n1                          Inconnu 2021-12-27   \n2                          Inconnu 2021-12-27   \n3                          Inconnu 2021-12-27   \n4                          Inconnu 2021-12-27   \n...                            ...        ...   \n1050                       Inconnu 2022-01-25   \n1057                       Inconnu 2022-01-25   \n1064  Télétravail partiel possible 2022-01-25   \n1065    Télétravail total possible 2022-01-25   \n1069                       Inconnu 2022-01-25   \n\n                                                   text  \\\n0     At Lyft, our mission is to improve people’s li...   \n1     Data Engineer\\nAbout Us\\nWith electric vehicle...   \n2     Delivering the best Spotify experience possibl...   \n3     Delivering the best Spotify experience possibl...   \n4     The Platform team creates the technology that ...   \n...                                                 ...   \n1050  We are the Global Savings Group, the leading E...   \n1057  BackMarket is the number one European (and soo...   \n1064  Avec VEEPEE, le groupe vente-privee ouvre un n...   \n1065  Stuart (DPD Group) is a sustainable 🌱 last-mil...   \n1069  Heetch: the VTC that keeps everyone moving. Ou...   \n\n                                         processed_text lang  length_text  \\\n0     lyft mission improve people life world best tr...   en         2672   \n1     data engineer u electric vehicle ev expected n...   en         3229   \n2     delivering best spotify experience possible ma...   en         4143   \n3     delivering best spotify experience possible ma...   en         5233   \n4     platform team creates technology enables spoti...   en         4186   \n...                                                 ...  ...          ...   \n1050  global saving group leading european commerce ...   en         3190   \n1057  backmarket number one european soon global mar...   en         4204   \n1064  veepee groupe venteprivee ouvre nouveau chapit...   en         5032   \n1065  stuart dpd group sustainable lastmile delivery...   en         4097   \n1069  heetch vtc keep everyone moving mission heetch...   en         5692   \n\n                                                technos  \n0     [S3, Flyte, ETL, Stackdriver, Kafka, Hive, Spa...  \n1     [mlflow, nodejs, airflow, Python, Airflow, Kub...  \n2     [Dataflow, Kubeflow, Pub/Sub, BigQuery, Apache...  \n3                                 [Scala, Python, Java]  \n4     [Scala, Google Cloud Platform, Python, Java, SQL]  \n...                                                 ...  \n1050  [S3, Hadoop, Scala, Flink, Glue, EMR, SQL, Spa...  \n1057  [NoSQL, Kafka, Go, Spark, Participating, Pytho...  \n1064  [Grafana, PostgreSQL, Kafka, Proficiency, Pyth...  \n1065  [S3, Redshift, Hadoop, Flink, Kafka, SparkSQL,...  \n1069  [Morocco, Algeria, airflow, SMS, Python, Looke...  \n\n[523 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>title</th>\n      <th>company</th>\n      <th>location</th>\n      <th>type</th>\n      <th>industry</th>\n      <th>remote</th>\n      <th>created_at</th>\n      <th>text</th>\n      <th>processed_text</th>\n      <th>lang</th>\n      <th>length_text</th>\n      <th>technos</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://datai.jobs/job/lyft-data-engineer-kyiv...</td>\n      <td>Data Engineer – Kyiv</td>\n      <td>Lyft</td>\n      <td>Kyiv, Ukraine</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>At Lyft, our mission is to improve people’s li...</td>\n      <td>lyft mission improve people life world best tr...</td>\n      <td>en</td>\n      <td>2672</td>\n      <td>[S3, Flyte, ETL, Stackdriver, Kafka, Hive, Spa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://datai.jobs/job/chargepoint-data-engine...</td>\n      <td>Data Engineer</td>\n      <td>ChargePoint</td>\n      <td>Amsterdam</td>\n      <td>Full Time</td>\n      <td>Vehicles &amp; Autonomous Mobility</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>Data Engineer\\nAbout Us\\nWith electric vehicle...</td>\n      <td>data engineer u electric vehicle ev expected n...</td>\n      <td>en</td>\n      <td>3229</td>\n      <td>[mlflow, nodejs, airflow, Python, Airflow, Kub...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://datai.jobs/job/spotify-data-engineer-e...</td>\n      <td>Data Engineer – Experience</td>\n      <td>Spotify</td>\n      <td>Stockholm</td>\n      <td>Full Time</td>\n      <td>Entertainment</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>Delivering the best Spotify experience possibl...</td>\n      <td>delivering best spotify experience possible ma...</td>\n      <td>en</td>\n      <td>4143</td>\n      <td>[Dataflow, Kubeflow, Pub/Sub, BigQuery, Apache...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://datai.jobs/job/spotify-staff-data-engi...</td>\n      <td>Staff Data Engineer – Experience</td>\n      <td>Spotify</td>\n      <td>Stockholm</td>\n      <td>Full Time</td>\n      <td>Entertainment</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>Delivering the best Spotify experience possibl...</td>\n      <td>delivering best spotify experience possible ma...</td>\n      <td>en</td>\n      <td>5233</td>\n      <td>[Scala, Python, Java]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://datai.jobs/job/spotify-data-engineer-s...</td>\n      <td>Data Engineer</td>\n      <td>Spotify</td>\n      <td>Stockholm</td>\n      <td>Full Time</td>\n      <td>Entertainment</td>\n      <td>Inconnu</td>\n      <td>2021-12-27</td>\n      <td>The Platform team creates the technology that ...</td>\n      <td>platform team creates technology enables spoti...</td>\n      <td>en</td>\n      <td>4186</td>\n      <td>[Scala, Google Cloud Platform, Python, Java, SQL]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1050</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer</td>\n      <td>GLOBAL SAVINGS GROUP</td>\n      <td>München</td>\n      <td>CDI</td>\n      <td>AdTech / MarTech, E-commerce, IT / Digital</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>We are the Global Savings Group, the leading E...</td>\n      <td>global saving group leading european commerce ...</td>\n      <td>en</td>\n      <td>3190</td>\n      <td>[S3, Hadoop, Scala, Flink, Glue, EMR, SQL, Spa...</td>\n    </tr>\n    <tr>\n      <th>1057</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer</td>\n      <td>Back Market</td>\n      <td>Bordeaux</td>\n      <td>CDI</td>\n      <td>Collaborative Economy, E-commerce, Environment...</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>BackMarket is the number one European (and soo...</td>\n      <td>backmarket number one european soon global mar...</td>\n      <td>en</td>\n      <td>4204</td>\n      <td>[NoSQL, Kafka, Go, Spark, Participating, Pytho...</td>\n    </tr>\n    <tr>\n      <th>1064</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer (Platform team)</td>\n      <td>Veepee</td>\n      <td>Paris</td>\n      <td>CDI</td>\n      <td>E-commerce</td>\n      <td>Télétravail partiel possible</td>\n      <td>2022-01-25</td>\n      <td>Avec VEEPEE, le groupe vente-privee ouvre un n...</td>\n      <td>veepee groupe venteprivee ouvre nouveau chapit...</td>\n      <td>en</td>\n      <td>5032</td>\n      <td>[Grafana, PostgreSQL, Kafka, Proficiency, Pyth...</td>\n    </tr>\n    <tr>\n      <th>1065</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data Engineer (Remote)</td>\n      <td>Stuart</td>\n      <td>N</td>\n      <td>CDI</td>\n      <td>Collaborative Economy, Logistics</td>\n      <td>Télétravail total possible</td>\n      <td>2022-01-25</td>\n      <td>Stuart (DPD Group) is a sustainable 🌱 last-mil...</td>\n      <td>stuart dpd group sustainable lastmile delivery...</td>\n      <td>en</td>\n      <td>4097</td>\n      <td>[S3, Redshift, Hadoop, Flink, Kafka, SparkSQL,...</td>\n    </tr>\n    <tr>\n      <th>1069</th>\n      <td>https://www.welcometothejungle.com/fr/companie...</td>\n      <td>Data engineer  business centric</td>\n      <td>Heetch</td>\n      <td>Paris</td>\n      <td>CDI</td>\n      <td>Mobile Apps, Mobility</td>\n      <td>Inconnu</td>\n      <td>2022-01-25</td>\n      <td>Heetch: the VTC that keeps everyone moving. Ou...</td>\n      <td>heetch vtc keep everyone moving mission heetch...</td>\n      <td>en</td>\n      <td>5692</td>\n      <td>[Morocco, Algeria, airflow, SMS, Python, Looke...</td>\n    </tr>\n  </tbody>\n</table>\n<p>523 rows × 13 columns</p>\n</div>"
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_jobs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [],
   "source": [
    "# // TODO Add other NER entities to avoid 'Morocco', 'Algeria'... to be labeled as technos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exporting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [],
   "source": [
    "english_jobs.to_csv('english_jobs_ner.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}